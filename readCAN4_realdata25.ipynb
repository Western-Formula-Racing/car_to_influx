{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Uploading CAN data to InfluxDB",
   "id": "c9dcf9892f9025c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import cantools\n",
    "from influxdb_client import Point, InfluxDBClient\n",
    "from influxdb_client.client.write_api import WriteOptions\n",
    "\n",
    "def parse_can_csv_row(row, db):\n",
    "    \"\"\"\n",
    "    Parse one CSV row dict, decode it via cantools, and return a list of InfluxDB Points.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        can_id_int = int(row['message_id'])\n",
    "    except (KeyError, ValueError):\n",
    "        return [f\"Error: Invalid CAN ID '{row.get('message_id')}'\"]\n",
    "\n",
    "    try:\n",
    "        message = db.get_message_by_frame_id(can_id_int)\n",
    "    except KeyError:\n",
    "        return [f\"Error: No message found for CAN ID {can_id_int}\"]\n",
    "\n",
    "    # collect the eight bytes (some may be empty strings)\n",
    "    data_list = bytes(\n",
    "        int(row[f'byte{b}']) for b in range(8)\n",
    "        if row.get(f'byte{b}') not in (None, '')\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        decoded = message.decode(data_list)\n",
    "    except Exception as e:\n",
    "        return [f\"Error: Decoding error for CAN ID {can_id_int} — {e}\"]\n",
    "\n",
    "    # parse the timestamp field into a UTC datetime\n",
    "    try:\n",
    "        ts = float(row['timestamp'])\n",
    "        ts_dt = datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "    except Exception:\n",
    "        ts_dt = datetime.now(timezone.utc)\n",
    "\n",
    "    points = []\n",
    "    for sig_name, raw in decoded.items():\n",
    "        sig = message.get_signal_by_name(sig_name)\n",
    "        desc = getattr(sig, 'comment', '') or \"No description\"\n",
    "        unit = getattr(sig, 'unit', '') or \"N/A\"\n",
    "        if hasattr(raw, 'value'):  # NamedSignalValue\n",
    "            val = float(raw.value)\n",
    "            label = raw.name\n",
    "        else:\n",
    "            val = float(raw)\n",
    "            label = str(raw)\n",
    "\n",
    "        pt = (\n",
    "            Point(\"canBus\")\n",
    "            .tag(\"signalName\", sig_name)\n",
    "            .tag(\"messageName\", message.name)\n",
    "            .tag(\"canId\", str(can_id_int))\n",
    "            .field(\"sensorReading\", val)\n",
    "            .field(\"unit\", unit)\n",
    "            .field(\"description\", desc)\n",
    "            .field(\"signalLabel\", label)\n",
    "            .time(ts_dt)\n",
    "        )\n",
    "        points.append(pt)\n",
    "    return points\n",
    "\n",
    "def process_csv(file_path, db, write_api, bucket, mps=400):\n",
    "    \"\"\"\n",
    "    Read CSV, decode at `mps` messages/sec, batch‑write to InfluxDB.\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for idx, row in enumerate(reader):\n",
    "            pts = parse_can_csv_row(row, db)\n",
    "            if pts and isinstance(pts[0], str) and pts[0].startswith(\"Error:\"):\n",
    "                print(f\"Row {idx}: {pts[0]}\")\n",
    "            else:\n",
    "                batch.extend(pts)\n",
    "                count += 1\n",
    "\n",
    "            if count >= mps:\n",
    "                if batch:\n",
    "                    write_api.write(bucket=bucket, org=\"WFR\", record=batch)\n",
    "                    batch = []\n",
    "                elapsed = time.time() - start\n",
    "                if elapsed < 1:\n",
    "                    time.sleep(1 - elapsed)\n",
    "                start = time.time()\n",
    "                count = 0\n",
    "\n",
    "    if batch:\n",
    "        write_api.write(bucket=bucket, org=\"WFR\", record=batch)\n",
    "\n",
    "def main():\n",
    "    # load DBC\n",
    "    dbc_file = 'WFR25-2.dbc'\n",
    "    try:\n",
    "        db = cantools.database.load_file(dbc_file)\n",
    "        print(f\"Loaded DBC: {dbc_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed loading DBC: {e}\")\n",
    "        return\n",
    "\n",
    "    # InfluxDB client setup\n",
    "    influx_url = \"http://3.98.181.12:8086\"\n",
    "    with open('influx_token.txt') as f:\n",
    "        token = f.read().strip()\n",
    "\n",
    "    client = InfluxDBClient(url=influx_url, token=token, org=\"WFR\")\n",
    "    write_api = client.write_api(write_options=WriteOptions(batch_size=10000,\n",
    "                                                            flush_interval=1000))\n",
    "\n",
    "    csv_path = 'local_analysis/cleaned_can.csv'\n",
    "    process_csv(csv_path, db, write_api, bucket=\"ourCar\", mps=10000)\n",
    "    print(\"Finished writing all points.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "34ef4081b2cc0b79",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
