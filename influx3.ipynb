{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:27:08.063063Z",
     "start_time": "2025-06-01T12:27:00.976799Z"
    }
   },
   "source": [
    "import influxdb_client_3\n",
    "from influxdb_client_3 import InfluxDBClient3, Point, WriteOptions"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:27:08.816382Z",
     "start_time": "2025-06-01T12:27:08.170740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = InfluxDBClient3(host=f\"http://3.98.181.12:9000\",\n",
    "                        database=f\"WFRtest\",\n",
    "                        token=f\"apiv3_gIW9ZiakRQi_JrfdaJA2Q7VBcUym95S76lbhmbn59dtmDKNk-yZRyZQwJcu7gElvn_1yWpRnbwwy-rjuwUbQMw\")"
   ],
   "id": "39031e0ad76d8a9b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:30:27.672556Z",
     "start_time": "2025-06-01T12:30:27.038143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "point = \"home,room=Living\\\\ Room temp=21.1,hum=35.9,co=0i 1748678400\"\n",
    "client.write(record=point, write_precision=\"s\")"
   ],
   "id": "bd78efed145a6fbf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Execute an SQL query\n",
    "table = client.query(query='''SELECT room\n",
    "                            FROM home\n",
    "                            WHERE temp=21.1\n",
    "                              AND time=from_unixtime(1748678400)''')\n",
    "# table is a pyarrow.Table\n",
    "room = table[0][0]\n",
    "assert f\"{room}\" == 'Living Room', f\"Expected {room} to be Living Room\""
   ],
   "id": "f8702f27bdb395cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"Room: {room}\")",
   "id": "ac327a72c8d31a89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Code below uploads to InfluxDB 3.0 using the InfluxDBClient3\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import cantools\n",
    "\n",
    "from influxdb_client_3 import InfluxDBClient3, Point, WriteOptions\n",
    "\n",
    "# Setup error logging\n",
    "logging.basicConfig(\n",
    "    filename='parse_errors.log',\n",
    "    filemode='w',\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "def parse_can_csv_row(row, db):\n",
    "    try:\n",
    "        can_id_int = int(row['message_id'])\n",
    "    except (KeyError, ValueError):\n",
    "        return [f\"Error: Invalid CAN ID '{row.get('message_id')}'\"] # Return list with error string\n",
    "\n",
    "    try:\n",
    "        message = db.get_message_by_frame_id(can_id_int)\n",
    "    except KeyError:\n",
    "        return [f\"Error: No message found for CAN ID {can_id_int}\"]\n",
    "\n",
    "    # Byte parsing - aiming to replicate your original logic's expectation\n",
    "    # This assumes all 'byte0' through 'byte7' fields are expected to be present in the CSV\n",
    "    # and contain valid integer strings.\n",
    "    byte_values_from_row = []\n",
    "    try:\n",
    "        for i in range(8):  # Expect byte0 through byte7\n",
    "            byte_str = row.get(f'byte{i}')\n",
    "            if byte_str is None or byte_str == '':\n",
    "                # Original code's `if len(data_list) < 8:` check implies all 8 bytes must be present.\n",
    "                # If a byte field is missing/empty, the original `data_list` would be shorter,\n",
    "                # causing the length check to fail.\n",
    "                return [f\"Error: Missing or empty value for byte{i} for CAN ID {can_id_int}. All 8 byte fields are expected.\"]\n",
    "            byte_values_from_row.append(int(byte_str)) # Can raise ValueError\n",
    "        data_for_decode = bytes(byte_values_from_row)\n",
    "    except ValueError as e: # From int() conversion\n",
    "        return [f\"Error: Non-integer byte value in CSV for CAN ID {can_id_int} (byte{i}: '{byte_str}') — {e}\"]\n",
    "    except Exception as e: # General catch for byte parsing phase\n",
    "        return [f\"Error: Failed byte parse for CAN ID {can_id_int} — {e}\"]\n",
    "\n",
    "    try:\n",
    "        decoded = message.decode(data_for_decode)\n",
    "    except Exception as e:\n",
    "        return [f\"Error: Decoding error for CAN ID {can_id_int} with data {data_for_decode.hex()} — {e}\"]\n",
    "\n",
    "    try:\n",
    "        ts_str = row['timestamp']\n",
    "        ts = float(ts_str)\n",
    "        ts_dt = datetime.fromtimestamp(ts, tz=timezone.utc)\n",
    "    except (KeyError, ValueError, TypeError):\n",
    "        ts_dt = datetime.now(timezone.utc) # Fallback timestamp\n",
    "\n",
    "    line_protocol_points = []\n",
    "    measurement_name = \"canBus\"\n",
    "\n",
    "    # Helper function for escaping characters in InfluxDB Line Protocol\n",
    "    def escape_lp_component(value, is_field_string_value=False):\n",
    "        s_value = str(value)\n",
    "        if is_field_string_value:\n",
    "            # For field string values: escape backslashes and double quotes\n",
    "            return s_value.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n",
    "        else:\n",
    "            # For measurement, tag keys, tag values, field keys:\n",
    "            # escape spaces, commas, and equal signs\n",
    "            s_value = s_value.replace(',', '\\\\,')\n",
    "            s_value = s_value.replace(' ', '\\\\ ')\n",
    "            s_value = s_value.replace('=', '\\\\=')\n",
    "            return s_value\n",
    "\n",
    "    # Pre-escape parts that are common to all signals in this message\n",
    "    escaped_message_name = escape_lp_component(message.name)\n",
    "    escaped_can_id_str = escape_lp_component(str(can_id_int))\n",
    "\n",
    "    for sig_name, raw_signal_value in decoded.items():\n",
    "        sig = message.get_signal_by_name(sig_name)\n",
    "        unit = getattr(sig, 'unit', '') or \"N/A\"\n",
    "\n",
    "        if hasattr(raw_signal_value, 'value'):  # It's an Enum-like object from cantools\n",
    "            numeric_val = float(raw_signal_value.value)\n",
    "            label_str = str(raw_signal_value.name)\n",
    "        else:  # It's a direct numeric value\n",
    "            numeric_val = float(raw_signal_value)\n",
    "            label_str = str(raw_signal_value)\n",
    "\n",
    "        # Escape components for Line Protocol\n",
    "        escaped_sig_name = escape_lp_component(sig_name)\n",
    "        escaped_unit_val = escape_lp_component(unit, is_field_string_value=True)\n",
    "        escaped_label_val = escape_lp_component(label_str, is_field_string_value=True)\n",
    "\n",
    "        # Timestamp in nanoseconds (InfluxDB default precision)\n",
    "        timestamp_ns = int(ts_dt.timestamp() * 1_000_000_000)\n",
    "\n",
    "        # Assemble tags string part: key1=value1,key2=value2\n",
    "        tags_str = f\"signalName={escaped_sig_name},messageName={escaped_message_name},canID={escaped_can_id_str}\"\n",
    "\n",
    "        # Assemble fields string part: key1=value1,key2=\"string value\"\n",
    "        # sensorReading is float, unit and signalLabel are strings (hence quoted)\n",
    "        fields_str = f'sensorReading={numeric_val},unit=\"{escaped_unit_val}\",signalLabel=\"{escaped_label_val}\"'\n",
    "        # If numeric_val could be NaN or Inf, f-string will produce 'nan', 'inf', '-inf', which is standard.\n",
    "\n",
    "        # Assemble the final line protocol string for this signal point\n",
    "        # Format: measurement,tag_set field_set timestamp\n",
    "        lp_point = f\"{measurement_name},{tags_str} {fields_str} {timestamp_ns}\"\n",
    "        line_protocol_points.append(lp_point)\n",
    "\n",
    "    return line_protocol_points\n",
    "\n",
    "\n",
    "\n",
    "def process_csv(file_path, db, influx_client, database_name, mps=400):\n",
    "    batch = []\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = list(csv.DictReader(csvfile))\n",
    "        total = len(reader)\n",
    "        for idx, row in enumerate(tqdm(reader, total=total, desc=\"Uploading CAN data\")):\n",
    "            pts = parse_can_csv_row(row, db)\n",
    "            if pts and isinstance(pts[0], str) and pts[0].startswith(\"Error:\"):\n",
    "                logging.error(f\"Row {idx}: {pts[0]}\")\n",
    "            else:\n",
    "                batch.extend(pts)\n",
    "                count += 1\n",
    "\n",
    "            if count >= mps:\n",
    "                if batch:\n",
    "                    influx_client.write(\n",
    "                        database=database_name,\n",
    "                        record=batch,\n",
    "                        write_precision=\"ns\",\n",
    "                        write_options=WriteOptions(batch_size=10000, flush_interval=100)\n",
    "                    )\n",
    "                    batch = []\n",
    "                elapsed = time.time() - start\n",
    "                if elapsed < 1:\n",
    "                    time.sleep(1 - elapsed)\n",
    "                start = time.time()\n",
    "                count = 0\n",
    "\n",
    "    if batch:\n",
    "        influx_client.write(\n",
    "            database=database_name,\n",
    "            record=batch,\n",
    "            write_precision=\"ns\",\n",
    "            write_options=WriteOptions(batch_size=10000, flush_interval=100)\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    dbc_file = 'local_analysis/WFR25-3.dbc'\n",
    "    try:\n",
    "        db = cantools.database.load_file(dbc_file)\n",
    "        print(f\"Loaded DBC: {dbc_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed loading DBC: {e}\")\n",
    "        return\n",
    "\n",
    "    influx_url = \"http://3.98.181.12:9000\"\n",
    "    influx_token = \"apiv3_gIW9ZiakRQi_JrfdaJA2Q7VBcUym95S76lbhmbn59dtmDKNk-yZRyZQwJcu7gElvn_1yWpRnbwwy-rjuwUbQMw\"\n",
    "    database_name = \"WFRtest\"\n",
    "\n",
    "    client = InfluxDBClient3(\n",
    "        host=influx_url,\n",
    "        database=database_name,\n",
    "        token=influx_token\n",
    "    )\n",
    "\n",
    "    csv_path = 'local_analysis/cleaned_can.csv'\n",
    "    process_csv(csv_path, db, client, database_name, mps=10000)\n",
    "\n",
    "    print(\"Finished writing all points.\")\n",
    "    print(\"Errors (if any) logged in parse_errors.log\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "e2ac4c1e930f92c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:57:35.496931Z",
     "start_time": "2025-06-01T08:57:35.262958Z"
    }
   },
   "cell_type": "code",
   "source": "result = client.query(query=\"SELECT * FROM home\", database=\"WFRtest\")",
   "id": "e025a5b614c00998",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:57:40.090942Z",
     "start_time": "2025-06-01T08:57:40.088034Z"
    }
   },
   "cell_type": "code",
   "source": "print(result)",
   "id": "1e98eba03e4b3dbf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "co: int64\n",
      "hum: double\n",
      "room: string\n",
      "temp: double\n",
      "time: timestamp[ns] not null\n",
      "----\n",
      "co: [[0]]\n",
      "hum: [[35.9]]\n",
      "room: [[\"Living Room\"]]\n",
      "temp: [[21.1]]\n",
      "time: [[2025-05-31 08:00:00.000000000]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9723691c5cb9c631"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
